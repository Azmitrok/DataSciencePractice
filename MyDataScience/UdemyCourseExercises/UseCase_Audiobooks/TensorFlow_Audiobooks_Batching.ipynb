{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audiobooks business case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create a class that will batch the data\n",
    "\n",
    "Whenever you want to batch the data you need to have appropriate methods. There are some batching methods integrated in TensorFlow and sklearn, but some problems may need specific coding. \n",
    "\n",
    "Here we show how these methods look like. You can use them for any machine learning framework you need (directly or after little fine-tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a class that will do the batching for the algorithm\n",
    "# This code is extremely reusable. You should just change Audiobooks_data everywhere in the code\n",
    "class Audiobooks_Data_Reader():\n",
    "    # Dataset is a mandatory arugment, while the batch_size is optional\n",
    "    # If you don't input batch_size, it will automatically take the value: None\n",
    "    def __init__(self, dataset, batch_size = None):\n",
    "    \n",
    "        # The dataset that loads is one of \"train\", \"validation\", \"test\".\n",
    "        # e.g. if I call this class with x('train',5), it will load 'Audiobooks_data_train.npz' with a batch size of 5.\n",
    "        npz = np.load('Audiobooks_data_{0}.npz'.format(dataset))\n",
    "        \n",
    "        # Two variables that take the values of the inputs and the targets. Inputs are floats, targets are integers\n",
    "        self.inputs, self.targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "        \n",
    "        # Counts the batch number, given the size you feed it later\n",
    "        # If the batch size is None, we are either validating or testing, so we want to take the data in a single batch\n",
    "        if batch_size is None:\n",
    "            self.batch_size = self.inputs.shape[0]\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        self.curr_batch = 0\n",
    "        self.batch_count = self.inputs.shape[0] // self.batch_size\n",
    "    \n",
    "    # A method which loads the next batch\n",
    "    def __next__(self):\n",
    "        if self.curr_batch >= self.batch_count:\n",
    "            self.curr_batch = 0\n",
    "            raise StopIteration()\n",
    "            \n",
    "        # You slice the dataset in batches and then the \"next\" function loads them one after the other\n",
    "        batch_slice = slice(self.curr_batch * self.batch_size, (self.curr_batch + 1) * self.batch_size)\n",
    "        inputs_batch = self.inputs[batch_slice]\n",
    "        targets_batch = self.targets[batch_slice]\n",
    "        self.curr_batch += 1\n",
    "        \n",
    "        # One-hot encode the targets. In this example it's a bit superfluous since we have a 0/1 column \n",
    "        # as a target already but we're giving you the code regardless, as it will be useful for any \n",
    "        # classification task with more than one target column\n",
    "        classes_num = 2\n",
    "        targets_one_hot = np.zeros((targets_batch.shape[0], classes_num))\n",
    "        targets_one_hot[range(targets_batch.shape[0]), targets_batch] = 1\n",
    "        \n",
    "        # The function will return the inputs batch and the one-hot encoded targets\n",
    "        return inputs_batch, targets_one_hot\n",
    "    \n",
    "        \n",
    "    # A method needed for iterating over the batches, as we will put them in a loop\n",
    "    # This tells Python that the class we're defining is iterable, i.e. that we can use it like:\n",
    "    # for input, output in data: \n",
    "        # do things\n",
    "    # An iterator in Python is a class with a method __next__ that defines exactly how to iterate through its objects\n",
    "    def __iter__(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Mean loss: 0.685. Validation loss: 0.658. Validation accuracy: 68.46%\n",
      "Epoch 2. Mean loss: 0.604. Validation loss: 0.544. Validation accuracy: 77.18%\n",
      "Epoch 3. Mean loss: 0.475. Validation loss: 0.442. Validation accuracy: 78.30%\n",
      "Epoch 4. Mean loss: 0.396. Validation loss: 0.395. Validation accuracy: 79.19%\n",
      "Epoch 5. Mean loss: 0.365. Validation loss: 0.375. Validation accuracy: 78.52%\n",
      "Epoch 6. Mean loss: 0.351. Validation loss: 0.364. Validation accuracy: 79.64%\n",
      "Epoch 7. Mean loss: 0.343. Validation loss: 0.357. Validation accuracy: 79.64%\n",
      "Epoch 8. Mean loss: 0.337. Validation loss: 0.354. Validation accuracy: 79.87%\n",
      "Epoch 9. Mean loss: 0.334. Validation loss: 0.350. Validation accuracy: 79.64%\n",
      "Epoch 10. Mean loss: 0.331. Validation loss: 0.347. Validation accuracy: 80.09%\n",
      "Epoch 11. Mean loss: 0.329. Validation loss: 0.346. Validation accuracy: 80.31%\n",
      "Epoch 12. Mean loss: 0.327. Validation loss: 0.344. Validation accuracy: 79.87%\n",
      "Epoch 13. Mean loss: 0.326. Validation loss: 0.343. Validation accuracy: 80.54%\n",
      "Epoch 14. Mean loss: 0.325. Validation loss: 0.341. Validation accuracy: 80.54%\n",
      "Epoch 15. Mean loss: 0.323. Validation loss: 0.341. Validation accuracy: 80.76%\n",
      "Epoch 16. Mean loss: 0.322. Validation loss: 0.338. Validation accuracy: 80.76%\n",
      "Epoch 17. Mean loss: 0.322. Validation loss: 0.339. Validation accuracy: 80.76%\n",
      "End of training\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "input_size = 10\n",
    "output_size = 2\n",
    "hidden_layer1_size = 150\n",
    "hidden_layer2_size = 100\n",
    "hidden_layer3_size = 50\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [None, input_size])\n",
    "targets = tf.placeholder(tf.int32, [None, output_size])\n",
    "\n",
    "weights_1 = tf.get_variable(\"weights_1\", [input_size, hidden_layer1_size])\n",
    "biases_1 = tf.get_variable(\"biases_1\", [hidden_layer1_size])\n",
    "\n",
    "outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1)+biases_1)\n",
    "\n",
    "weights_2 = tf.get_variable(\"weights_2\", [hidden_layer1_size,hidden_layer2_size])\n",
    "biases_2 = tf.get_variable(\"biases_2\", [hidden_layer2_size])\n",
    "\n",
    "outputs_2 = tf.nn.softmax(tf.matmul(outputs_1, weights_2) + biases_2)\n",
    "\n",
    "weights_3 = tf.get_variable(\"weights_3\", [hidden_layer2_size,hidden_layer3_size])\n",
    "biases_3 = tf.get_variable(\"biases_3\", [hidden_layer3_size])\n",
    "\n",
    "outputs_3 = tf.nn.tanh(tf.matmul(outputs_2, weights_3) + biases_3)\n",
    "\n",
    "weights_4 = tf.get_variable(\"weights_4\", [hidden_layer3_size,output_size])\n",
    "biases_4 = tf.get_variable(\"biases_4\", [output_size])\n",
    "\n",
    "outputs = tf.matmul(outputs_3, weights_4) + biases_4\n",
    "\n",
    "#activation function\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits = outputs, labels = targets)\n",
    "\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "optimize = tf.train.AdamOptimizer(learning_rate=0.001).minimize(mean_loss)\n",
    "\n",
    "out_equals_target = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "initializer = tf.global_variables_initializer()\n",
    "\n",
    "sess.run(initializer)\n",
    "\n",
    "batch_size =  100\n",
    "\n",
    "max_epochs = 50\n",
    "\n",
    "prev_validation_loss = 9999999.\n",
    "\n",
    "train_data = Audiobooks_Data_Reader('train', batch_size)\n",
    "validation_data = Audiobooks_Data_Reader('validation')\n",
    "\n",
    "for epoch_counter in range(max_epochs):\n",
    "    #print(\"Epoch start:\" + str(epoch_counter))\n",
    "    \n",
    "    curr_epoch_loss = 0.\n",
    "    batch_counter = 0\n",
    "    for input_batch, target_batch in train_data:\n",
    "        batch_counter +=1\n",
    "        #print(\"batch counter\" + str(batch_counter))        \n",
    "        _, batch_loss = sess.run([optimize, mean_loss], feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        \n",
    "        curr_epoch_loss += batch_loss\n",
    "    \n",
    "    curr_epoch_loss /= train_data.batch_count\n",
    "    \n",
    "    validation_loss = 0.\n",
    "    validation_accuracy = 0.\n",
    "    \n",
    "    for input_batch, target_batch in validation_data:\n",
    "        validation_loss, validation_accuracy = sess.run([mean_loss, accuracy],\n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "                \n",
    "    print('Epoch '+str(epoch_counter+1)+\n",
    "          '. Mean loss: '+'{0:.3f}'.format(curr_epoch_loss)+\n",
    "          '. Validation loss: '+'{0:.3f}'.format(validation_loss)+\n",
    "          '. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.)+'%')\n",
    "    \n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break\n",
    "    \n",
    "    prev_validation_loss = validation_loss\n",
    "\n",
    "print('End of training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:           82.37%\n"
     ]
    }
   ],
   "source": [
    "test_data = Audiobooks_Data_Reader('test')\n",
    "\n",
    "for input_batch, target_batch in test_data:\n",
    "    test_accuracy = sess.run([accuracy], feed_dict={inputs:input_batch, targets: target_batch})\n",
    "    \n",
    "print(\"Test accuracy:\" + '{0: >#016.2f}'.format(test_accuracy[0]*100)+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, \"tmp/model1.ckpt\")\n",
    "print(\"Model saved in path: %s\" % save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
